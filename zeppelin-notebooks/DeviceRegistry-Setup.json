{"paragraphs":[{"text":"val keyId = \"dummy\"\nval secret = \"dummy\"\nsc.hadoopConfiguration.set(\"fs.s3n.awsAccessKeyId\", keyId)\nsc.hadoopConfiguration.set(\"fs.s3n.awsSecretAccessKey\", secret)\nsc.hadoopConfiguration.set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n//val fs = \"s3n://hwx-randy/\"\nval fs = \"/Users/randy/demos/techops/data/\"\n\nimport sqlContext.implicits._\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.types.{StructType,StructField,StringType};\nimport java.text.SimpleDateFormat\n\ndef cacheJSON(table: String) :Unit ={\n  val name = table.replace(\"-\", \"_\") + \"_raw\"\n  sqlContext.sql(\"drop table if exists \" + name)\n  sqlContext.read.json(fs+table+\"/*\").createOrReplaceTempView(name+\"_tmp\")\n  sqlContext.sql(\"select input_file_name() as fn, * from \" + name + \"_tmp\").coalesce(1).write.format(\"parquet\")\n    .mode(\"overwrite\").option(\"path\", fs+\"derived/\"+name).saveAsTable(name)\n  println(\"Wrote \" + name + \" to \" + fs+\"derived/\"+name)\n  sqlContext.cacheTable(name)\n  println(\"Cached \" + name)\n  sqlContext.sql(\"drop table \" + name + \"_tmp\")\n}\n\ndef cacheTable(query: String, table: String) :Unit = {\n  sqlContext.sql(\"drop table if exists \" + table)\n  sqlContext.sql(query).coalesce(1).write.format(\"parquet\")\n    .mode(\"overwrite\").option(\"path\", fs+\"derived/\"+table).saveAsTable(table)\n  println(\"Wrote \" + table + \" to \"+fs+\"derived/\"+table)\n  sqlContext.cacheTable(table)\n  println(\"Cached table \"+table)\n}","dateUpdated":"2016-08-14T22:37:17-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1465838747781_365131195","id":"20160613-132547_1349290144","result":{"code":"SUCCESS","type":"TEXT","msg":"\nkeyId: String = dummy\n\nsecret: String = dummy\n\nfs: String = /Users/randy/demos/techops/data/\n\nimport sqlContext.implicits._\n\nimport org.apache.spark.sql.Row\n\nimport org.apache.spark.sql.types.{StructType, StructField, StringType}\n\nimport java.text.SimpleDateFormat\n\ncacheJSON: (table: String)Unit\n\ncacheTable: (query: String, table: String)Unit\n"},"dateCreated":"2016-06-13T01:25:47-0400","dateStarted":"2016-08-14T22:37:17-0400","dateFinished":"2016-08-14T22:37:19-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2210"},{"title":"Parse Raw JSON","text":"cacheJSON(\"nodes\")\ncacheJSON(\"node-health\")\ncacheJSON(\"services\")\ncacheJSON(\"service-health\")","dateUpdated":"2016-08-14T22:37:17-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true,"tableHide":false,"title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1466288158271_-1327105944","id":"20160618-181558_825401827","result":{"code":"SUCCESS","type":"TEXT","msg":"Wrote nodes_raw to /Users/randy/demos/techops/data/derived/nodes_raw\nCached nodes_raw\nWrote node_health_raw to /Users/randy/demos/techops/data/derived/node_health_raw\nCached node_health_raw\nWrote services_raw to /Users/randy/demos/techops/data/derived/services_raw\nCached services_raw\nWrote service_health_raw to /Users/randy/demos/techops/data/derived/service_health_raw\nCached service_health_raw\n"},"dateCreated":"2016-06-18T06:15:58-0400","dateStarted":"2016-08-14T22:37:17-0400","dateFinished":"2016-08-14T22:37:37-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2211"},{"text":"cacheTable(\"\"\"\nselect\n  from_unixtime(split(split(fn, '/')[size(split(fn, '/'))-1], '\\\\.')[0]/1000) as datetime,\n  *\nfrom nodes_raw\n\"\"\", \"nodes\")","dateUpdated":"2016-08-14T22:37:17-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1466664793245_-892923142","id":"20160623-025313_2076088888","result":{"code":"SUCCESS","type":"TEXT","msg":"Wrote nodes to /Users/randy/demos/techops/data/derived/nodes\nCached table nodes\n"},"dateCreated":"2016-06-23T02:53:13-0400","dateStarted":"2016-08-14T22:37:20-0400","dateFinished":"2016-08-14T22:37:38-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2212"},{"text":"cacheTable(\"\"\"\nselect datetime, service.service, service.id, b.address, service.port, check.node, check.checkid, check.name, check.status\nfrom\n(select\n  from_unixtime(split(split(fn, '/')[size(split(fn, '/'))-1], '\\\\.')[0]/1000) as datetime,\n  explode(checks) as check,\n  node,\n  service\nfrom service_health_raw) a\nleft outer join (\n  select distinct node, address from nodes\n) b on check.node = b.node\n\"\"\", \"services\")","dateUpdated":"2016-08-14T22:37:17-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1466496537843_332586327","id":"20160621-040857_468046896","result":{"code":"SUCCESS","type":"TEXT","msg":"Wrote services to /Users/randy/demos/techops/data/derived/services\nCached table services\n"},"dateCreated":"2016-06-21T04:08:57-0400","dateStarted":"2016-08-14T22:37:38-0400","dateFinished":"2016-08-14T22:37:39-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2213"},{"text":"val IP_PATTERN = \"(([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.){3}([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\"\nval columns = Array(\"ts\",\"IP\", \"log\")\nval schema = StructType(columns.map(x => StructField(x, StringType, true)))\n\ncase class weblog(ts: String, IP: String, log: String)\n\nsqlContext.sql(\"select distinct service, collect_set(concat('\"+fs+\"host-data/logs/',node)) from services where service = 'web' group by service\").collect()\n  .map(x => (x.getString(0), x.getSeq[String](1).mkString(\"/*/*,\"))).map(y => {\n    val name = y._1+\"_logs_raw\"\n    //this represents a source-specific parser for 'web' logs. Supply your own parser for other type logs\n    sc.textFile(y._2).map(_.split(\"\\t\").map(_.trim)).map(z => weblog(z(0), z(1), z(2))).toDF().coalesce(1).createOrReplaceTempView(\"web_logs_raw\")\n  })","dateUpdated":"2016-08-14T22:37:17-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1466421380144_-1547809689","id":"20160620-071620_1752966747","result":{"code":"SUCCESS","type":"TEXT","msg":"\nIP_PATTERN: String = (([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.){3}([01]?\\d\\d?|2[0-4]\\d|25[0-5])\n\ncolumns: Array[String] = Array(ts, IP, log)\n\nschema: org.apache.spark.sql.types.StructType = StructType(StructField(ts,StringType,true), StructField(IP,StringType,true), StructField(log,StringType,true))\n\ndefined class weblog\n\nres26: Array[Unit] = Array(())\n"},"dateCreated":"2016-06-20T07:16:20-0400","dateStarted":"2016-08-14T22:37:39-0400","dateFinished":"2016-08-14T22:37:40-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2214"},{"text":"cacheTable(\"\"\"\nselect\n  from_unixtime(unix_timestamp(ts, \"dd/MMM/yyyy HH:mm:ss\")) as datetime,\n  split(input_file_name(), \"/\")[size(split(input_file_name(), \"/\"))-3] as node,\n  ip as source_ip,\n  log \nfrom web_logs_raw\n\"\"\", \"web_logs\")","dateUpdated":"2016-08-14T22:37:18-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"datetime","index":0,"aggr":"sum"}],"values":[{"name":"node","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"datetime","index":0,"aggr":"sum"}}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1466651969647_-1086039288","id":"20160622-231929_821088003","result":{"code":"SUCCESS","type":"TEXT","msg":"Wrote web_logs to /Users/randy/demos/techops/data/derived/web_logs\nCached table web_logs\n"},"dateCreated":"2016-06-22T11:19:29-0400","dateStarted":"2016-08-14T22:37:40-0400","dateFinished":"2016-08-14T22:37:40-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2215"},{"text":"cacheTable(\"\"\"\nselect web_logs.datetime, web_logs.node as app_host, source_ip, b.node as source_host, log\nfrom web_logs\nleft outer join (select distinct node, address from nodes) b on source_ip = address\n\"\"\", \"web_logs_enriched\")","dateUpdated":"2016-08-14T22:37:18-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":true,"keys":[{"name":"datetime","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"datetime","index":0,"aggr":"sum"}}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1466525078071_1536121999","id":"20160621-120438_1014423901","result":{"code":"SUCCESS","type":"TEXT","msg":"Wrote web_logs_enriched to /Users/randy/demos/techops/data/derived/web_logs_enriched\nCached table web_logs_enriched\n"},"dateCreated":"2016-06-21T12:04:38-0400","dateStarted":"2016-08-14T22:37:40-0400","dateFinished":"2016-08-14T22:37:41-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2216"},{"text":"val nmon_raw = sc.wholeTextFiles(fs+\"host-data/metrics/nmon/*\")\ncase class nmonRec(node: String, ts: String, CPUUser: Double, CPUSys: Double, MemFree: Long, MemAvailable: Long, MemTotal: Long, DiskBusy: Double)\n\nnmon_raw.map(x => nmonRec(\n    x._1.split(\"/\").last.split(\"_\")(0), //node\n    x._2.split(\"\\n\").filter(_ contains \"ZZZZ\")(0).split(\",\")(3) + \" \" +\n      x._2.split(\"\\n\").filter(_ contains \"ZZZZ\")(0).split(\",\")(2), //timestamp\n    x._2.split(\"\\n\").filter(x => x.contains(\"T0001\") && x.startsWith(\"CPU_ALL\"))(0).split(\",\")(2).trim.toDouble, // User CPU\n    x._2.split(\"\\n\").filter(x => x.contains(\"T0001\") && x.startsWith(\"CPU_ALL\"))(0).split(\",\")(3).trim.toDouble, // Sys CPU\n    x._2.split(\"\\n\").filter(x => x.contains(\"MemFree\"))(0).split(\": \")(1).split(\" kB\")(0).trim.toLong,\n    x._2.split(\"\\n\").filter(x => x.contains(\"MemAvailable\"))(0).split(\": \")(1).split(\" kB\")(0).trim.toLong,\n    x._2.split(\"\\n\").filter(x => x.contains(\"MemTotal\"))(0).split(\": \")(1).split(\" kB\")(0).trim.toLong,\n    x._2.split(\"\\n\").filter(x => x.contains(\"DISKBUSY\"))(1).split(\",\")(2).trim.toDouble\n)).toDF().coalesce(1).createOrReplaceTempView(\"nmon_raw\")","dateUpdated":"2016-08-14T22:37:18-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1467502781509_-570766126","id":"20160702-193941_1940694652","result":{"code":"SUCCESS","type":"TEXT","msg":"\nnmon_raw: org.apache.spark.rdd.RDD[(String, String)] = /Users/randy/demos/techops/data/host-data/metrics/nmon/* MapPartitionsRDD[333] at wholeTextFiles at <console>:43\n\ndefined class nmonRec\n"},"dateCreated":"2016-07-02T07:39:41-0400","dateStarted":"2016-08-14T22:37:41-0400","dateFinished":"2016-08-14T22:37:42-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2217"},{"text":"cacheTable(\"\"\"\nselect from_unixtime(unix_timestamp(ts, \"dd-MMM-yyyy HH:mm:ss\")) as datetime, node, cpuuser, cpusys, memfree, memavailable, memtotal, diskbusy from nmon_raw\n\"\"\", \"node_monitoring\")","dateUpdated":"2016-08-14T22:37:18-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":true,"keys":[{"name":"datetime","index":0,"aggr":"sum"}],"values":[{"name":"node","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"datetime","index":0,"aggr":"sum"},"yAxis":{"name":"node","index":1,"aggr":"sum"}}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1466289364898_1826783650","id":"20160618-183604_920305267","result":{"code":"ERROR","type":"TEXT","msg":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norg.apache.spark.SparkException: Job aborted.\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:149)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:115)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:60)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:58)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:86)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:86)\n  at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:487)\n  at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:246)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:60)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:58)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:86)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:86)\n  at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:378)\n  at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:354)\n  at cacheTable(<console>:44)\n  ... 50 elided\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 51.0 failed 1 times, most recent failure: Lost task 0.0 in stage 51.0 (TID 22674, localhost): org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 0\n\tat $anonfun$1.apply(<console>:50)\n\tat $anonfun$1.apply(<console>:48)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$12.next(Iterator.scala:444)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438)\n\tat org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253)\n\tat org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252)\n\tat org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1325)\n\tat org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:258)\n\t... 8 more\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1904)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:143)\n  ... 81 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows\n  at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n  at org.apache.spark.scheduler.Task.run(Task.scala:85)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n  ... 3 more\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 0\n  at $anonfun$1.apply(<console>:50)\n  at $anonfun$1.apply(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<console>:48)\n  at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n  at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n  at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n  at scala.collection.Iterator$$anon$12.next(Iterator.scala:444)\n  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)\n  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438)\n  at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253)\n  at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252)\n  at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252)\n  at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1325)\n  at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:258)\n  ... 8 more\n"},"dateCreated":"2016-06-18T06:36:04-0400","dateStarted":"2016-08-14T22:37:41-0400","dateFinished":"2016-08-14T22:37:45-0400","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:2218"},{"text":"val ps_raw = sc.wholeTextFiles(fs+\"host-data/metrics/ps/*\")\ncase class Process(node: String, ts: Long, user: String, pid: String, CPU: Double, Mem: Double, VSZ: Int, RSS: Int, TTY: String, STAT: String, Start: String, Time: String, Command: String)\n\n//Take the timestamp from filename and prepend it to each row of \"ps\" output\nps_raw.flatMap(x => {\n  val node = x._1.split(\"/\")(x._1.split(\"/\").size - 2)\n  val ts = x._1.split(\"/\").last\n  x._2.split(\"\\n\").drop(1).map(node + \" \" + ts + \" \" + _)\n}).map(y => { \n  val x = y.split(\"\\\\s+\")\n  Process(\n    x(0), x(1).toLong/1000, x(2), x(3), x(4).toDouble, x(5).toDouble, x(6).toInt, x(7).toInt, x(8), x(9), x(10), x(11), x.slice(12, x.size-1).mkString(\" \")\n  )\n}).toDF().coalesce(1).createOrReplaceTempView(\"ps_raw\")\n  \n//TODO - fix timestamp issue","dateUpdated":"2016-08-14T22:37:18-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1468242729194_-1404414465","id":"20160711-091209_1121889648","result":{"code":"SUCCESS","type":"TEXT","msg":"\nps_raw: org.apache.spark.rdd.RDD[(String, String)] = /Users/randy/demos/techops/data/host-data/metrics/ps/* MapPartitionsRDD[343] at wholeTextFiles at <console>:43\n\ndefined class Process\n"},"dateCreated":"2016-07-11T09:12:09-0400","dateStarted":"2016-08-14T22:37:42-0400","dateFinished":"2016-08-14T22:37:46-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2219"},{"text":"val ns_raw = sc.wholeTextFiles(fs+\"host-data/metrics/netstat/*\")\ncase class Socket(node: String, ts: Long, Proto: String, RecvQ: Int, SendQ: Int, LocalAddress: String, ForeignAddress: String, State: String, User: String, Inode: String, pid: String, ProgramName: String)\n\n//Take the timestamp from filename and prepend it to each row of \"netstat\" output\nns_raw.flatMap(x => {\n  val node = x._1.split(\"/\")(x._1.split(\"/\").size - 2)\n  val ts = x._1.split(\"/\").last\n  x._2.split(\"\\n\").drop(1).map(node + \" \" + ts + \" \" + _)\n}).filter(x => !(x contains \"Recv-Q\") && !(x contains \"Active UNIX\") && !(x contains \"RefCnt\") && !(x.split(\"\\\\s+\")(2) contains \"unix\") && !(x contains \"Active Internet\"))\n  .map(y => {\n  val x = y.split(\"\\\\s+\")\n  Socket(\n    x(0), x(1).toLong/1000, x(2), x(3).toInt, x(4).toInt, x(5), x(6), x(7), x(8), x(9), if (x.size > 11) x(10) else \"\", if (x.size > 12) x(11) else \"\"\n  )\n})\n.toDF().coalesce(1).createOrReplaceTempView(\"netstat_raw\")\n\ncase class IPCSocket(node: String, ts: Long, Proto: String, RefCnt: Int, Flags: String, Type: String, State: String, Inode: String, pid: String, ProgamName: String, Path: String)\nns_raw.flatMap(x => {\n  val node = x._1.split(\"/\")(x._1.split(\"/\").size - 2)\n  val ts = x._1.split(\"/\").last\n  x._2.split(\"\\n\").drop(1).map(node + \" \" + ts + \" \" + _)\n}).filter(x => x contains \"unix\")\n  .map(y => {\n  val x = y.split(\"\\\\s+\")\n  IPCSocket(\n    x(0), x(1).toLong/1000, x(2), x(3).toInt, x.slice(4,x.size-4).mkString(\" \"), x(x.size-4), x(x.size-3), x(x.size-2), x.last.split(\"/\")(0), x.last.split(\"/\")(1), \"\"\n  )\n})\n.toDF().coalesce(1).createOrReplaceTempView(\"ipcsocket_raw\")\n//TODO - fix timestamp issue","dateUpdated":"2016-08-14T22:37:18-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"ts","index":0,"aggr":"sum"}],"values":[{"name":"_c1","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"ts","index":0,"aggr":"sum"},"yAxis":{"name":"_c1","index":1,"aggr":"sum"}}},"enabled":true,"editorMode":"ace/mode/scala","helium":{},"editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1468244810103_-1947338967","id":"20160711-094650_1171392473","result":{"code":"SUCCESS","type":"TEXT","msg":"\nns_raw: org.apache.spark.rdd.RDD[(String, String)] = /Users/randy/demos/techops/data/host-data/metrics/netstat/* MapPartitionsRDD[349] at wholeTextFiles at <console>:43\n\ndefined class Socket\n\ndefined class IPCSocket\n"},"dateCreated":"2016-07-11T09:46:50-0400","dateStarted":"2016-08-14T22:37:45-0400","dateFinished":"2016-08-14T22:37:47-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2220"},{"text":"%sql\ncreate table if not exists metrics as\nselect\n  concat(date_format(datetime, 'yyyy-MM-dd HH:mm'), ':00') as time,\n  round(avg(cpuuser + cpusys)) as CPUUsed,\n  round(avg(diskbusy)) as DISKUsed,\n  round(avg(memavailable/memtotal)*100) as RAMUsed,\n  round(avg(memavailable)/1000) as MemAvailable,\n  sum(b.requests) as InternalRequests,\n  sum(c.requests) as ExternalRequests,\n  sum((b.requests + c.requests)) as TotalRequests\nfrom node_monitoring a\njoin (\n  select\n    date_format(datetime, 'yyyy-MM-dd HH:mm') as time,\n    1 as requests\n  from web_logs_enriched\n  where source_host is not null or source_ip = '127.0.0.1'\n) b on date_format(a.datetime, 'yyyy-MM-dd HH:mm') = b.time\njoin (\n  select \n    date_format(datetime, 'yyyy-MM-dd HH:mm') as time,\n    1 as requests\n  from web_logs_enriched\n  where source_host is null  \n) c on date_format(a.datetime, 'yyyy-MM-dd HH:mm') = c.time\ngroup by date_format(datetime, 'yyyy-MM-dd HH:mm')\norder by time asc","dateUpdated":"2016-08-14T22:37:18-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sql","editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1467523425109_-459526821","id":"20160703-012345_2004558290","result":{"code":"ERROR","type":"TEXT","msg":"org.apache.spark.sql.AnalysisException: Table or view not found: node_monitoring; line 11 pos 5\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:449)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:468)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:453)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:60)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:321)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:319)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:58)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:321)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:319)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:58)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:321)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:319)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:58)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:321)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:319)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:58)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:321)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:319)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:58)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:453)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:443)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:51)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:582)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:682)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.zeppelin.spark.SparkSqlInterpreter.interpret(SparkSqlInterpreter.java:115)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:94)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:383)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:176)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n"},"dateCreated":"2016-07-03T01:23:45-0400","dateStarted":"2016-08-14T22:37:46-0400","dateFinished":"2016-08-14T22:37:47-0400","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:2221"},{"text":"","dateUpdated":"2016-08-14T22:37:18-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1470783876410_-280699832","id":"20160809-190436_699902842","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2016-08-09T07:04:36-0400","dateStarted":"2016-08-14T22:37:47-0400","dateFinished":"2016-08-14T22:37:47-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2222"}],"name":"DeviceRegistry-Setup","id":"2BQZRZCQB","lastReplName":{"value":"sql"},"angularObjects":{"2BQ4DA89C:shared_process":[],"2BQ2F53M6:shared_process":[],"2BSY8XK83:shared_process":[],"2BQQW1DA2:shared_process":[],"2BTD7M9Z7:shared_process":[],"2BRQE418G:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}